{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Robust Classifier Against Adversarial Examples\n",
    "\n",
    "This notebook trains a robust classifier against adversarial examples like FGSM and PGD attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "First, let's load and prepare the training dataset. We'll also create validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset (assuming this is the distribution)\n",
    "# Replace with actual dataset loading if different\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "We'll use ResNet50 as our base architecture since it provides a good balance between capacity and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name='resnet50', num_classes=10):\n",
    "    if model_name == 'resnet18':\n",
    "        model = resnet18(pretrained=True)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = resnet34(pretrained=True)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = resnet50(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported. Use resnet18, resnet34, or resnet50.\")\n",
    "    \n",
    "    # Modify the first convolution layer to accept 3-channel 32x32 inputs (CIFAR-10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # Remove maxpool to preserve spatial dimensions for the small input size\n",
    "    model.maxpool = nn.Identity()\n",
    "    \n",
    "    # Modify the final fully connected layer for our number of classes\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Choose one of the required models\n",
    "model_name = 'resnet50'  # You can try different models (resnet18, resnet34, resnet50)\n",
    "model = get_model(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Adversarial Attack Methods\n",
    "\n",
    "Now, let's implement FGSM (Fast Gradient Sign Method) and PGD (Projected Gradient Descent) attacks for adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack\n",
    "def fgsm_attack(model, images, labels, epsilon=0.1):\n",
    "    # Make sure gradients are calculated\n",
    "    images.requires_grad = True\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Create perturbation\n",
    "    data_grad = images.grad.data\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    \n",
    "    # Create adversarial example\n",
    "    perturbed_images = images + epsilon * sign_data_grad\n",
    "    \n",
    "    # Clamp to ensure valid pixel range [0,1]\n",
    "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    \n",
    "    return perturbed_images\n",
    "\n",
    "# PGD Attack\n",
    "def pgd_attack(model, images, labels, epsilon=0.1, alpha=0.01, num_iter=10):\n",
    "    perturbed_images = images.clone().detach()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        perturbed_images.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(perturbed_images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        if perturbed_images.grad is not None:\n",
    "            perturbed_images.grad.data.zero_()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Create single-step perturbation\n",
    "        data_grad = perturbed_images.grad.data\n",
    "        adv_images = perturbed_images.detach() + alpha * data_grad.sign()\n",
    "        \n",
    "        # Project back to epsilon ball\n",
    "        eta = torch.clamp(adv_images - images, -epsilon, epsilon)\n",
    "        perturbed_images = torch.clamp(images + eta, 0, 1).detach()\n",
    "    \n",
    "    return perturbed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function with Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, validloader, epochs=10, lr=0.01, epsilon=0.03, adv_ratio=0.5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_clean_acc': [],\n",
    "        'val_fgsm_acc': [],\n",
    "        'val_pgd_acc': []\n",
    "    }\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(tqdm(trainloader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Random mix of clean and adversarial examples for training\n",
    "            if np.random.random() < adv_ratio:\n",
    "                # Choose randomly between FGSM and PGD attacks\n",
    "                if np.random.random() < 0.5:\n",
    "                    # FGSM attack\n",
    "                    perturbed_inputs = fgsm_attack(model, inputs, targets, epsilon)\n",
    "                else:\n",
    "                    # PGD attack\n",
    "                    perturbed_inputs = pgd_attack(model, inputs, targets, epsilon)\n",
    "            else:\n",
    "                perturbed_inputs = inputs\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(perturbed_inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        clean_acc, fgsm_acc, pgd_acc = evaluate_model(model, validloader, epsilon)\n",
    "        \n",
    "        # Save metrics to history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_clean_acc'].append(clean_acc)\n",
    "        history['val_fgsm_acc'].append(fgsm_acc)\n",
    "        history['val_pgd_acc'].append(pgd_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        avg_adv_acc = (fgsm_acc + pgd_acc) / 2\n",
    "        if avg_adv_acc > best_acc:\n",
    "            best_acc = avg_adv_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_name': model_name,\n",
    "                'clean_acc': clean_acc,\n",
    "                'fgsm_acc': fgsm_acc,\n",
    "                'pgd_acc': pgd_acc,\n",
    "            }, f'best_{model_name}_robust.pt')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader, epsilon=0.03):\n",
    "    model.eval()\n",
    "    \n",
    "    # Clean accuracy\n",
    "    correct_clean = 0\n",
    "    total = 0\n",
    "    \n",
    "    # FGSM accuracy\n",
    "    correct_fgsm = 0\n",
    "    \n",
    "    # PGD accuracy\n",
    "    correct_pgd = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            # Clean accuracy\n",
    "            outputs_clean = model(inputs)\n",
    "            _, predicted_clean = outputs_clean.max(1)\n",
    "            correct_clean += predicted_clean.eq(targets).sum().item()\n",
    "            \n",
    "            # FGSM accuracy\n",
    "            fgsm_inputs = fgsm_attack(model, inputs, targets, epsilon)\n",
    "            outputs_fgsm = model(fgsm_inputs)\n",
    "            _, predicted_fgsm = outputs_fgsm.max(1)\n",
    "            correct_fgsm += predicted_fgsm.eq(targets).sum().item()\n",
    "            \n",
    "            # PGD accuracy\n",
    "            pgd_inputs = pgd_attack(model, inputs, targets, epsilon)\n",
    "            outputs_pgd = model(pgd_inputs)\n",
    "            _, predicted_pgd = outputs_pgd.max(1)\n",
    "            correct_pgd += predicted_pgd.eq(targets).sum().item()\n",
    "    \n",
    "    clean_acc = 100 * correct_clean / total\n",
    "    fgsm_acc = 100 * correct_fgsm / total\n",
    "    pgd_acc = 100 * correct_pgd / total\n",
    "    \n",
    "    print(f'Clean Accuracy: {clean_acc:.2f}%')\n",
    "    print(f'FGSM  Accuracy: {fgsm_acc:.2f}%')\n",
    "    print(f'PGD   Accuracy: {pgd_acc:.2f}%')\n",
    "    \n",
    "    return clean_acc, fgsm_acc, pgd_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Validation data size: {len(val_dataset)}\")\n",
    "print(f\"Test data size: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with adversarial training\n",
    "trained_model, history = train_model(\n",
    "    model=model,\n",
    "    trainloader=train_loader,\n",
    "    validloader=val_loader,\n",
    "    epochs=20,            # Adjust as needed\n",
    "    lr=0.01,              # Initial learning rate\n",
    "    epsilon=0.03,         # Perturbation magnitude\n",
    "    adv_ratio=0.5         # Ratio of adversarial examples in training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot 2: Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_clean_acc'], label='Clean Val Acc')\n",
    "plt.plot(history['val_fgsm_acc'], label='FGSM Val Acc')\n",
    "plt.plot(history['val_pgd_acc'], label='PGD Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "clean_acc, fgsm_acc, pgd_acc = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Clean Accuracy: {clean_acc:.2f}%\")\n",
    "print(f\"FGSM Accuracy: {fgsm_acc:.2f}%\")\n",
    "print(f\"PGD Accuracy: {pgd_acc:.2f}%\")\n",
    "print(f\"Average Adversarial Accuracy: {(fgsm_acc + pgd_acc) / 2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare and Save Final Model for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "checkpoint = torch.load(f'best_{model_name}_robust.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Save the final model for submission\n",
    "submission_path = f'submission_{model_name}.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_name': model_name\n",
    "}, submission_path)\n",
    "\n",
    "print(f\"Model saved to {submission_path}\")\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Submit Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_model(file_path):\n",
    "    url = \"http://149.156.182.9:6060/task-3/submit\"\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        files = {'model': f}\n",
    "        response = requests.post(url, files=files)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Submission successful!\")\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(f\"Submission failed with status code: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "# Uncomment to submit\n",
    "# submit_model(submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Experiments (Optional)\n",
    "\n",
    "To improve robustness further, you might want to try:\n",
    "1. Different epsilon values for adversarial training\n",
    "2. Different model architectures (ResNet18, ResNet34)\n",
    "3. Ensemble methods combining multiple models\n",
    "4. Different PGD configurations (more iterations, different step sizes)\n",
    "5. Other regularization techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
